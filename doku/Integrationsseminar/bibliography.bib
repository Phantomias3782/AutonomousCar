% This file was created with Citavi 6.7.0.0

@misc{AutoPilotReview.2019,
 author = {{AutoPilot Review}},
 year = {2019},
 title = {Elon Musk on Cameras vs. LiDAR for Self Driving and Autonomous Cars},
 url = {https://www.youtube.com/watch?v=HM23sjhtk4Q&feature=youtu.be},
 urldate = {15.12.2020}
}


@book{Dr.Ing.ThomasTille.2018,
 author = {{Dr.-Ing. Thomas Tille}},
 year = {2018},
 title = {Automobil-Sensorik 2: Systeme, Technologien und Applikationen},
 url = {https://link.springer.com/chapter/10.1007/978-3-662-56310-6_2},
 address = {Springer Verlag},
 urldate = {15.12.2020},
 isbn = {978-3-662-56309-0},
 doi = {10.1007/978-3-662-56310-6}
}


@misc{Girshick.30.04.2015,
 abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
 author = {Girshick, Ross},
 date = {30.04.2015},
 title = {Fast R-CNN},
 url = {https://arxiv.org/pdf/1504.08083},
 file = {98d30d36-b5b4-47aa-ba8b-676ee3185ede:C\:\\Users\\andre\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\kxoljq2aw8tm2kb6epit982oqn0qag1gpr66bt0mwg8\\Citavi Attachments\\98d30d36-b5b4-47aa-ba8b-676ee3185ede.pdf:pdf}
}


@article{Girshick.2016,
 abstract = {Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
 author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
 year = {2016},
 title = {Region-Based Convolutional Networks for Accurate Object Detection and Segmentation},
 pages = {142--158},
 volume = {38},
 number = {1},
 issn = {1939-3539},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 doi = {10.1109/TPAMI.2015.2437384},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26656583}
}


@article{Manojkrishna.2018,
 abstract = {Image classification using Deep learning},
 author = {{Manoj krishna}, M. and Neelima, M. and Harshali, M. and {Venu Gopala Rao}, M.},
 year = {2018},
 title = {Image classification using Deep learning},
 pages = {614},
 volume = {7},
 number = {2.7},
 issn = {2227-524X},
 journal = {International Journal of Engineering {\&} Technology},
 doi = {10.14419/ijet.v7i2.7.10892},
 file = {3ca71018-d3e4-4e3b-807e-888f96c53686:C\:\\Users\\andre\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\kxoljq2aw8tm2kb6epit982oqn0qag1gpr66bt0mwg8\\Citavi Attachments\\3ca71018-d3e4-4e3b-807e-888f96c53686.pdf:pdf}
}


@misc{Ren.04.06.2015,
 abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
 author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
 date = {04.06.2015},
 title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal  Networks},
 url = {https://arxiv.org/pdf/1506.01497},
 file = {1777b957-a51f-45e4-ad91-7890913c8323:C\:\\Users\\andre\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\kxoljq2aw8tm2kb6epit982oqn0qag1gpr66bt0mwg8\\Citavi Attachments\\1777b957-a51f-45e4-ad91-7890913c8323.pdf:pdf}
}


@article{Uijlings.2013,
 abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99~{\%} recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software:                   http://disi.unitn.it/{\~{}}uijlings/SelectiveSearch.html                                  ).},
 author = {Uijlings, J. R. R. and {van de Sande}, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
 year = {2013},
 title = {Selective Search for Object Recognition},
 pages = {154--171},
 volume = {104},
 number = {2},
 issn = {1573-1405},
 journal = {International Journal of Computer Vision},
 doi = {10.1007/s11263-013-0620-5},
 file = {5b5c8df8-9dbc-4658-97bd-79f92c9c54e7:C\:\\Users\\andre\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\kxoljq2aw8tm2kb6epit982oqn0qag1gpr66bt0mwg8\\Citavi Attachments\\5b5c8df8-9dbc-4658-97bd-79f92c9c54e7.pdf:pdf}
}


@misc{Wang.18.12.2018,
 abstract = {3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies --- a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that it is not the quality of the data but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations --- essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance --- raising the detection accuracy of objects within the 30m range from the previous state-of-the-art of 22{\%} to an unprecedented 74{\%}. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo-image-based approaches. Our code is publicly available at https://github.com/mileyan/pseudo{\_}lidar.},
 author = {Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
 date = {18.12.2018},
 title = {Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object  Detection for Autonomous Driving},
 url = {https://arxiv.org/pdf/1812.07179},
 file = {0af98f84-6f0c-41ce-90a4-6f9cc6b8f17e:C\:\\Users\\andre\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\kxoljq2aw8tm2kb6epit982oqn0qag1gpr66bt0mwg8\\Citavi Attachments\\0af98f84-6f0c-41ce-90a4-6f9cc6b8f17e.pdf:pdf}
}

@misc{Jia.21.06.2014,
 abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ($\approx$ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
 author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
 date = {21.06.2014},
 title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
 url = {https://arxiv.org/pdf/1408.5093},
 file = {1cf31f8c-f2c9-461e-a55f-645de77e27d4:C\:\\Users\\andre\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\kxoljq2aw8tm2kb6epit982oqn0qag1gpr66bt0mwg8\\Citavi Attachments\\1cf31f8c-f2c9-461e-a55f-645de77e27d4.pdf:pdf}
}

@misc{Liu.08.12.2015,
 abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300$\backslash$times 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500$\backslash$times 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
 author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
 date = {2016},
 title = {SSD: Single Shot MultiBox Detector},
 url = {https://arxiv.org/pdf/1512.02325},
 doi = {10.1007/978-3-319-46448-0{\textunderscore }2},
 file = {075fb2fe-46d0-453e-a21b-22ed41d29f59:C\:\\Users\\andre\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\kxoljq2aw8tm2kb6epit982oqn0qag1gpr66bt0mwg8\\Citavi Attachments\\075fb2fe-46d0-453e-a21b-22ed41d29f59.pdf:pdf}
}

@misc{Redmon.08.06.2015,
 abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
 author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
 date = {08.06.2015},
 title = {You Only Look Once: Unified, Real-Time Object Detection},
 url = {https://arxiv.org/pdf/1506.02640},
 file = {75d3b0d2-fd55-4e3b-8aa3-a065ce5b7537:C\:\\Users\\andre\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\kxoljq2aw8tm2kb6epit982oqn0qag1gpr66bt0mwg8\\Citavi Attachments\\75d3b0d2-fd55-4e3b-8aa3-a065ce5b7537.pdf:pdf}
}


@misc{Redmon.09.04.2018,
 abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
 author = {Redmon, Joseph and Farhadi, Ali},
 date = {09.04.2018},
 title = {YOLOv3: An Incremental Improvement},
 url = {https://arxiv.org/pdf/1804.02767},
 file = {e0197203-6edd-482b-b910-d793ad962f8d:C\:\\Users\\andre\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\kxoljq2aw8tm2kb6epit982oqn0qag1gpr66bt0mwg8\\Citavi Attachments\\e0197203-6edd-482b-b910-d793ad962f8d.pdf:pdf}
}

@misc{RedmonJosephFahradiAli.2020,
 author = {{Redmon, Joseph, Fahradi, Ali}},
 year = {2020},
 title = {YOLO},
 url = {https://pjreddie.com/darknet/yolo/},
 urldate = {19.12.2020}
}

@misc{DarshanAdakane.2019,
 author = {{Darshan Adakane}},
 year = {2019},
 title = {Object Detection with OpenCV Python using YOLOv3},
 url = {https://medium.com/analytics-vidhya/object-detection-with-opencv-python-using-yolov3-481f02c6aa35},
 urldate = {19.12.2020}
}

@misc{DarshanAdakane.2019b,
 author = {{Darshan Adakane}},
 year = {2019},
 title = {Real Time Object Detection using YOLOv3 with OpenCV and Python},
 url = {https://medium.com/analytics-vidhya/real-time-object-detection-using-yolov3-with-opencv-and-python-64c985e14786},
 urldate = {19.12.2020}
}

@misc{AdrianRosebrock.2015,
 author = {{Adrian Rosebrock}},
 year = {2015},
 title = {Find distance from camera to object / marker using Python and OpenCV},
 url = {https://www.pyimagesearch.com/2015/01/19/find-distance-camera-objectmarker-using-python-opencv/},
 urldate = {19.12.2020}
}

@misc{PlaymoDB.2020,
 author = {PlaymoDB},
 year = {2020},
 title = {PlaymoDB},
 url = {https://playmodb.org/cgi-bin/category.pl?category=Klicky-Male%20child}
}

@misc{Rafi.2019,
 author = {Rafi},
 year = {2019},
 title = {Train your own tiny YOLO v3 on Google colaboratory with the custom dataset},
 url = {https://medium.com/@today.rafi/train-your-own-tiny-yolo-v3-on-google-colaboratory-with-the-custom-dataset-2e35db02bf8f}
}

@misc{behera_2019, title={Lane Line Detection}, url={https://www.kaggle.com/soumya044/lane-line-detection/notebook}, journal={Kaggle}, publisher={Kaggle}, author={Behera, Soumya Ranjan}, year={2019}, month={Sep}} 
 
@misc{OpenCV.2020, title={OpenCV Documentation}, url={https://opencv.org/about/}, publisher={OpenCV}, author={}, year={2020}}
 
@Inbook{Priese2015,
author="Priese, Lutz",
title="Einleitung",
bookTitle="Computer Vision: Einf{\"u}hrung in die Verarbeitung und Analyse digitaler Bilder",
year="2015",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--9",
abstract="In diesem Kapitel werden kurz die Teilgebiete Bildverarbeitung, Computergrafik und Visualisierung der Computervisualistik unterschieden. Die Philosophie des Autors und die Inhalte des Buches werden vorgestellt und die Inhalte diverser anderer Lehrb{\"u}chern werden in wenigen Stichw{\"o}rtern kurz genannt.",
isbn="978-3-662-45129-8",
doi="10.1007/978-3-662-45129-8_1",
url="https://doi.org/10.1007/978-3-662-45129-8_1"
}

@misc{cnnld_2020,
title={Robust Lane Detection from Continuous Driving Scenes Using Deep Neural Networks},
url={https://arxiv.org/pdf/1903.02193.pdf},
journal={IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY},
publisher={arxiv.org},
author= {{Qin Zou, Hanwen Jiang, Qiyu Dai, Yuanhao Yue, Long Chen, and Qian Wang}},
year={2020},
month={apr}}